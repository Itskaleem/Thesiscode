{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Configuration = {'RootPath': r\"C:\\Users\\Utente\\Projects\\Thesis\",\n",
    "                 'PositiveSamples':[r\"C:\\Users\\Utente\\Projects\\Thesis\\dataset\\Recaptured\"],\n",
    "                 'NegativeSamples':[r\"C:\\Users\\Utente\\Projects\\Thesis\\dataset\\Original\"],\n",
    "                 'PostiveSamples_dlc':[r'C:\\Users\\Utente\\Projects\\Thesis\\dataset\\DLC2021\\Recaptured'],\n",
    "                 'NegativeSamples_dlc': [r'C:\\Users\\Utente\\Projects\\Thesis\\dataset\\DLC2021\\Original'],\n",
    "                 'Destination_train':r\"C:\\Users\\Utente\\Projects\\Thesis\\Evaluations\\Machine Learning Based\\Extracted_features\\train\",\n",
    "                 'Destination_test':r\"C:\\Users\\Utente\\Projects\\Thesis\\Evaluations\\Machine Learning Based\\Extracted_features\\test\",\n",
    "                 'LBPF_parameters': [(8,1),(16,2),(24,3),(24,4)],\n",
    "                 'Methods': ['LBPF','MSWF','MARKOV','CNN'],\n",
    "                 'Batchsize':16,\n",
    "                 'Train_size': 0.7,\n",
    "                 'val_size' : 0.15,\n",
    "                 'test_size' : 0.15,\n",
    "                 'device': 'cuda' if torch.cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('Experiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Classes\n",
    "\n",
    "In this section, we will define the classes for the following feature extraction techniques:\n",
    "\n",
    "1. **Multi-Scale Local Binary Pattern (LBP) Features**\n",
    "2. **Multi-Scale Wavelet Features**\n",
    "3. **Markov Discrete Cosine Transform (DCT) Features**\n",
    "4. **ResNet18 Features**\n",
    "\n",
    "The use of All these features apart from ResNet was inspired by the paper https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462205\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Scale Local Binary Pattern (LBP) Features\n",
    "\n",
    "\n",
    "The implementation was inspired by the paper https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1017623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MultiScaleLocalBinaryPatternFeatures():\n",
    "    def __init__(self,parameters):\n",
    "        self.parameters = parameters\n",
    "\n",
    "    def Calculate_MultiScale_LBPF(self,image):\n",
    "        img_gray = cv2.cvtColor(image, \n",
    "                        cv2.COLOR_BGR2GRAY) \n",
    "        center_crop = self.get_center_block(img_gray)\n",
    "        hist = []\n",
    "        for param in self.parameters:\n",
    "            hist.extend(self.calc_LBP_hist(center_crop,param[0],param[1]))\n",
    "\n",
    "        return hist\n",
    "\n",
    "    def calc_LBP_hist(self,image, P, R):\n",
    "        lbp = feature.local_binary_pattern(image, P, R, method=\"uniform\")\n",
    "        hist = self.histogram(lbp, P, R)\n",
    "        return hist\n",
    "\n",
    "    def histogram(self,lbp, P, R):\n",
    "        (hist, _) = np.histogram(lbp.ravel(),\n",
    "                                bins=np.arange(0, P + 3),\n",
    "                                range=(0, P + 2))\n",
    "\n",
    "        # normalize the histogram\n",
    "        hist = np.float32(hist)\n",
    "        eps = 1e-7\n",
    "        hist /= (hist.sum() + eps)\n",
    "        return hist\n",
    "    \n",
    "    def get_center_block(self,image):\n",
    "        # Get the dimensions of the image\n",
    "        height, width = image.shape\n",
    "\n",
    "        # Calculate the starting and ending indices for the center block\n",
    "        start_row = (height - 224) // 2\n",
    "        end_row = start_row + 224\n",
    "        start_col = (width - 128) // 2\n",
    "        end_col = start_col + 224\n",
    "\n",
    "        # Extract the center block\n",
    "        center_block = image[start_row:end_row, start_col:end_col]\n",
    "\n",
    "        return center_block   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Here We will define the Class for MultiScaleWaveletFeatures.\n",
    "\n",
    "The code for this implementation was inspired by this paper, https://ieeexplore.ieee.org/document/5495419\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pywt\n",
    "\n",
    "\n",
    "class NlevelWaveletDecomposition():\n",
    "    def __init__(self,level=3,filter_type='haar'):\n",
    "        self.decomposition_level=level\n",
    "        self.filter= filter_type\n",
    "        \n",
    "    \n",
    "    def compute_wavelet_statistics(self,image):\n",
    "        \"\"\"\n",
    "        Compute wavelet statistics for the given image.\n",
    "\n",
    "        Parameters:\n",
    "            image (numpy.ndarray): Input image (RGB).\n",
    "            num_levels (int): Number of levels for wavelet decomposition.\n",
    "\n",
    "        Returns:\n",
    "            features (numpy.ndarray): Computed wavelet statistics features.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Convert image to float\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        # Separate R, G, B channels\n",
    "        channels = cv2.split(image)\n",
    "\n",
    "        # Initialize features array\n",
    "        features = []\n",
    "\n",
    "        # Perform wavelet decomposition for each channel\n",
    "        for channel in channels:\n",
    "            center_crop=self.get_center_block(channel)\n",
    "            coeffs = pywt.wavedec2(center_crop, self.filter , level=self.decomposition_level)\n",
    "\n",
    "            # Extract high-frequency bands and compute statistics\n",
    "            for i in range(1, self.decomposition_level + 1):\n",
    "                for detail_band in coeffs[i]:\n",
    "                    mean = np.mean(np.abs(detail_band))\n",
    "                    std_dev = np.std(np.abs(detail_band))\n",
    "                    features.extend([mean, std_dev])\n",
    "\n",
    "        return np.array(features)\n",
    "    \n",
    "    def get_center_block(self,image):\n",
    "        # Get the dimensions of the image\n",
    "        height, width = image.shape\n",
    "\n",
    "        # Calculate the starting and ending indices for the center block\n",
    "        start_row = (height - 224) // 2\n",
    "        end_row = start_row + 224\n",
    "        start_col = (width - 224) // 2\n",
    "        end_col = start_col + 224\n",
    "\n",
    "        # Extract the center block\n",
    "        center_block = image[start_row:end_row, start_col:end_col]\n",
    "\n",
    "        return center_block   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Markov Precess Based on Discrete Cosine Transform features\n",
    "\n",
    "\n",
    "The implementation of MarkoveDCT features was not found easily therefore with the help of google and youtube I cam with following implementation. however the approach is described in this paper\n",
    "\n",
    "https://dl.acm.org/doi/10.1145/2393347.2396396\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import dctn\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "class MarkovFeatures:\n",
    "    def __init__(self,threshold=3):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "\n",
    "    def compute_markov_features(self,image):\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "        Red,_,_ = cv2.split(image) \n",
    "\n",
    "        # Ensure image dimensions are multiples of 8\n",
    "        # Apply 2-D discrete cosine transform (DCT) to every 8x8 block\n",
    "        features = []\n",
    "\n",
    "        block = self.get_center_block(Red)\n",
    "        \n",
    "        dct_block = np.round(dctn(block)).astype(np.int32)\n",
    "        Fh, Fv, Fd, Fmd = self.compute_differences_dct_coefficientes(dct_block)\n",
    "        for f in [Fh,Fv,Fd,Fmd]:\n",
    "            transition_matrix =  self.compute_transition_probability_matrix(f)\n",
    "            features.extend(transition_matrix)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def get_center_block(self,image):\n",
    "        # Get the dimensions of the image\n",
    "        height, width = image.shape\n",
    "\n",
    "        # Calculate the starting and ending indices for the center block\n",
    "        start_row = (height - 8) // 2\n",
    "        end_row = start_row + 8\n",
    "        start_col = (width - 8) // 2\n",
    "        end_col = start_col + 8\n",
    "\n",
    "        # Extract the center block\n",
    "        center_block = image[start_row:end_row, start_col:end_col]\n",
    "\n",
    "        return center_block       \n",
    "\n",
    "    def compute_differences_dct_coefficientes(self,F):\n",
    "        # Compute the size of the image\n",
    "        Du, Dv = F.shape\n",
    "        \n",
    "        # Initialize difference DCT coefficients arrays\n",
    "        Fh = np.zeros((Du - 1, Dv))\n",
    "        Fv = np.zeros((Du, Dv - 1))\n",
    "        Fd = np.zeros((Du - 1, Dv - 1))\n",
    "        Fmd = np.zeros((Du - 1, Dv - 1))\n",
    "        \n",
    "        # Compute difference DCT coefficients arrays\n",
    "        for u in range(Du - 1):\n",
    "            for v in range(Dv):\n",
    "                Fh[u, v] = F[u, v] - F[u + 1, v]\n",
    "        \n",
    "        for u in range(Du):\n",
    "            for v in range(Dv - 1):\n",
    "                Fv[u, v] = F[u, v] - F[u, v + 1]\n",
    "        \n",
    "        for u in range(Du - 1):\n",
    "            for v in range(Dv - 1):\n",
    "                Fd[u, v] = F[u, v] - F[u + 1, v + 1]\n",
    "        \n",
    "        for u in range(Du - 1):\n",
    "            for v in range(Dv - 1):\n",
    "                Fmd[u, v] = F[u + 1, v] - F[u, v + 1]\n",
    "        \n",
    "        return Fh, Fv, Fd, Fmd\n",
    "    \n",
    "    def compute_transition_probability_matrix(self,Fx):\n",
    "        \n",
    "        # Apply thresholding technique\n",
    "        Fx_thresholded = np.ones_like(Fx)\n",
    "        for i in range(Fx.shape[0]):\n",
    "            for j in range(Fx.shape[1]):\n",
    "                if Fx[i][j] > self.threshold:\n",
    "                    Fx_thresholded[i][j] = self.threshold\n",
    "                elif Fx[i][j] < -self.threshold:\n",
    "                    Fx_thresholded[i][j] = -self.threshold\n",
    "                else:  # Include values within the threshold range\n",
    "                    Fx_thresholded[i][j] = Fx[i][j]\n",
    "    \n",
    "        m = n = 2 * self.threshold + 1\n",
    "        ph = np.zeros((m, n))\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                ph[i, j] = self.probability_ij(i - self.threshold - 1, j - self.threshold - 1, Fx_thresholded)\n",
    "\n",
    "        return ph.flatten()\n",
    "\n",
    "    def probability_ij(self, i, j, Fx_thresholded):\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "\n",
    "        u, v = Fx_thresholded.shape\n",
    "\n",
    "        for u_idx in range(u - 1):\n",
    "            for v_idx in range(v - 1):\n",
    "                if Fx_thresholded[u_idx][v_idx] == i and Fx_thresholded[u_idx + 1][v_idx] == j:\n",
    "                    numerator += 1\n",
    "\n",
    "                if Fx_thresholded[u_idx][v_idx] == i:\n",
    "                    denominator += 1\n",
    "\n",
    "        if denominator == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ResNet18-50 Features\n",
    "\n",
    "The class ResnetFeatures will help to obtain resent features from the encoder of choice. pytorch implemenation is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "\n",
    "class ResnetFeatures:\n",
    "    def __init__(self,model_name,device):\n",
    "\n",
    "        # Define preprocessing\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.CenterCrop((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        \n",
    "        self.device = device\n",
    "        # Download model\n",
    "        match model_name:\n",
    "            case 'ResNet50':\n",
    "                resnet = models.resnet50(weights='DEFAULT',progress=True)\n",
    "                resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "            case 'ResNet18':\n",
    "                resnet = models.resnet50(weights='DEFAULT',progress=True)\n",
    "                resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "\n",
    "        self.feature_extractor = resnet\n",
    "        self.feature_extractor.to(self.device)\n",
    "\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False        \n",
    "\n",
    "        self.feature_extractor.eval()\n",
    "\n",
    "\n",
    "    def get_features(self,images):\n",
    "\n",
    "        transformed_images = [self.preprocess(image) for image in images]\n",
    "        transformed_images= torch.stack(transformed_images).to(self.device)\n",
    "        output = self.feature_extractor(transformed_images)\n",
    "        results = [torch.flatten(feature, 0).cpu().numpy().tolist() for feature in output]\n",
    "\n",
    "        return results\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifer Class\n",
    "\n",
    "Classifier defined below is designed in a modular way to be able to use all previousely defined features and contains the pipleline for precess of loading the samples , splitting the data, shuffeling the data, logging , standarizing etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score,f1_score\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class Classification:\n",
    "    def __init__(self,transformation,identifier,positive_samples_path,negative_samples_path,writer):\n",
    "\n",
    "        self.transformation = transformation\n",
    "        self.positive_samples_path = positive_samples_path\n",
    "        self.negative_samples_path = negative_samples_path\n",
    "        self.identifier = identifier\n",
    "        self.writer = writer\n",
    "\n",
    "\n",
    "    def load_samples(self,test_docs):\n",
    "\n",
    "        positive_label = 1\n",
    "        negative_label = 0\n",
    "        positive_train_samples , positive_train_labels , positive_test_samples , positive_test_labels= self.load_positive_negative_samples(test_docs,self.positive_samples_path , positive_label)\n",
    "\n",
    "        negative_train_samples , negative_train_labels , negative_test_samples , negative_test_labels = self.load_positive_negative_samples(test_docs , self.negative_samples_path , negative_label)\n",
    "\n",
    "        return self.shuffle_data(pd.DataFrame({'samples':positive_train_samples+negative_train_samples,'labels':positive_train_labels+negative_train_labels})),pd.DataFrame({'samples':positive_test_samples+negative_test_samples,'labels':positive_test_labels+negative_test_labels})\n",
    "    \n",
    "\n",
    "    def load_positive_negative_samples(self,test_docs,sample_path , label ):\n",
    "\n",
    "        train_samples , train_labels , test_samples , test_labels = [] , [] , [] , []\n",
    "        if 'ResNet' in self.identifier:\n",
    "            idx = 0\n",
    "            images = []\n",
    "            batchsize = Configuration['Batchsize']\n",
    "\n",
    "        for path in sample_path:\n",
    "            for root, _, files in os.walk(path):\n",
    "                flag = 'train'\n",
    "                for i in test_docs:\n",
    "                    if i in root:\n",
    "                        flag = 'test'\n",
    "                \n",
    "                for file in files:\n",
    "                    if file.endswith((\".jpg\", \".png\")):  # Use tuple for multiple extensions\n",
    "                        path = os.path.join(root, file)\n",
    "                        image = cv2.imread(path)\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                        if 'ResNet' in self.identifier :\n",
    "                            if  idx<batchsize:\n",
    "                                images.append(image)\n",
    "                                idx += 1\n",
    "                                continue\n",
    "                            else:\n",
    "                                features=self.transformation(images)\n",
    "                                if flag=='train':\n",
    "                                    train_samples.extend(features)\n",
    "                                    train_labels.extend([label]*batchsize)\n",
    "                                else:\n",
    "                                    test_samples.extend(features)\n",
    "                                    test_labels.extend([label]*batchsize)\n",
    "                                idx=0\n",
    "                                images=[]\n",
    "                        \n",
    "                        else:\n",
    "                            feature = self.transformation(image)\n",
    "                            if flag=='train':\n",
    "                                train_samples.append(feature)\n",
    "                                train_labels.append(label)\n",
    "                            else:\n",
    "                                \n",
    "                                test_samples.append(feature)\n",
    "                                test_labels.append(label)\n",
    "        \n",
    "\n",
    "\n",
    "        return train_samples,train_labels,test_samples,test_labels\n",
    "    \n",
    "\n",
    "\n",
    "    def shuffle_data(self,data):\n",
    "        shuffeled_data = data.sample(frac=1, random_state=42)\n",
    "        return shuffeled_data\n",
    "\n",
    "\n",
    "    def split_data(self,data):\n",
    "               \n",
    "        train_data, val_test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "        \n",
    "        val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "        return train_data,val_data,test_data\n",
    "        \n",
    "    def train_model(self,best_params,train_samples,train_labels):\n",
    "        \n",
    "        if best_params['model_type'] == 'randomforest':\n",
    "            model = RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth'],min_samples_split=best_params['min_sample_split'],min_samples_leaf=best_params['min_samples_leaf'])\n",
    "            model.fit(train_samples,train_labels)       \n",
    "        else:\n",
    "            model = SVC(kernel=best_params['kernel'], C=best_params['C'],gamma=best_params['gamma'])\n",
    "            model.fit(train_samples,train_labels)\n",
    "       \n",
    "        return model \n",
    "    \n",
    "    def test_model(self,model,test_data,test_labels):\n",
    "        y_pred = model.predict(test_data)\n",
    "        accuracy = accuracy_score(y_pred, test_labels)\n",
    "        precision = precision_score(test_labels,y_pred)\n",
    "        recall = recall_score(test_labels,y_pred)\n",
    "        f1_scoree = f1_score(test_labels,y_pred,average='macro')\n",
    "\n",
    "        results_string = \"Results for {}: \\n\".format(self.identifier)\n",
    "        results_string += f'Results on Features: {self.identifier} are below:\\n'\n",
    "        results_string += \"Accuracy: {}\\n\".format(accuracy)\n",
    "        results_string += \"Precision: {}\\n\".format(precision)\n",
    "        results_string += \"Recall: {}\\n\".format(recall)\n",
    "        results_string += \"F1 Macro Score: {}\\n\".format(f1_scoree)\n",
    "\n",
    "        return results_string\n",
    "\n",
    "    \n",
    "    def load_data(self,pickle_file_path):\n",
    "        with open(pickle_file_path,'rb') as handle:\n",
    "            df=pd.read_pickle(handle)\n",
    "        return df\n",
    "    \n",
    "    def save_data(self,destination_path,data):\n",
    "        with open(os.path.join(destination_path,self.identifier+'.pkl'),'wb') as handle:\n",
    "            pickle.dump(data,handle)\n",
    "\n",
    "\n",
    "    def get_data_distributions(self,train_data,test_data):\n",
    "\n",
    "        train_dict = train_data['labels'].value_counts().to_dict()\n",
    "        test_dict = test_data['labels'].value_counts().to_dict()\n",
    "\n",
    "        print(f'Training Data Distribution: \\n{train_dict}')\n",
    "        print(f'Training Data Distribution: \\n{test_dict}')\n",
    "\n",
    "        self.writer.add_text(f'{self.identifier} Train Distribution',f'Training Data Distribution: \\n{train_dict}',0)\n",
    "        self.writer.add_text(f'{self.identifier} Test Distribution',f'Training Data Distribution: \\n{train_dict}',0)\n",
    "\n",
    "        return train_dict,test_dict\n",
    "\n",
    "    def standardize_data(self, training_data,test_data):\n",
    "\n",
    "        scaler = preprocessing.StandardScaler().fit(training_data['samples'].tolist())\n",
    "        training_data_scaled = scaler.transform(training_data['samples'].tolist())\n",
    "        test_data_scaled = scaler.transform(test_data['samples'].tolist())\n",
    "\n",
    "        return training_data_scaled.tolist(),test_data_scaled.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Based on Different Features And Model Selection and Evaluation on Same Distribution dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Local Binary Pattern Features\n",
    "\n",
    "Below we will try to obtain Local binary pattern features then with the help of optuna libray find the best performing model along with the best, Then evaluate the chosed model with chosen parametes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "LBPF = MultiScaleLocalBinaryPatternFeatures(Configuration['LBPF_parameters']).Calculate_MultiScale_LBPF\n",
    "\n",
    "classifier = Classification(LBPF,'LBPF',Configuration['PositiveSamples'],Configuration['NegativeSamples'],writer=writer)\n",
    "\n",
    "\n",
    "# if the features are already computed use load_data function else user load_samples which will first compute the features\n",
    "\n",
    "train_data = classifier.load_data(r\"C:\\Users\\Utente\\Projects\\Thesis\\Evaluations\\Machine Learning Based\\Extracted_features\\train\\LBPF.pkl\")\n",
    "test_data = classifier.load_data(r\"C:\\Users\\Utente\\Projects\\Thesis\\Evaluations\\Machine Learning Based\\Extracted_features\\test\\LBPF.pkl\")\n",
    "\n",
    "\n",
    "'''\n",
    "#if you perform load samples you would probably wanna save the features for not computing the features again and again in order to not lose precious time\n",
    "\n",
    "classifier.save_data(Configuration['Destination_train'],train_data)\n",
    "classifier.save_data(Configuration['Destination_test'],test_data)\n",
    "print('Data saved')\n",
    "'''\n",
    "\n",
    "classifier.get_data_distributions(train_data,test_data)\n",
    "print('distribution achieved')\n",
    "\n",
    "training_data_scaled, test_data_scaled = classifier.standardize_data(train_data,test_data)\n",
    "\n",
    "train_data['samples'] = training_data_scaled\n",
    "test_data['samples'] = test_data_scaled\n",
    "\n",
    "print('data Standarized')\n",
    "\n",
    "def model_performance(model):\n",
    "    \"\"\"\n",
    "    Get accuracy score on validation/test data from a trained model\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(test_data['samples'].tolist())\n",
    "    return round(accuracy_score(y_pred, test_data['labels'].tolist()),3)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    model = 'svm'\n",
    "    model_type = trial.suggest_categorical('model_type', ['randomforest', 'svm'])\n",
    "    if model_type == 'svm':\n",
    "        kernel = trial.suggest_categorical('kernel', ['poly', 'rbf', 'sigmoid'])\n",
    "        C = trial.suggest_float('C', 10, 210,step=10)\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"auto\", \"scale\"])\n",
    "\n",
    "        model = SVC(kernel=kernel, C=C,gamma=gamma)\n",
    "\n",
    "    if model_type == 'randomforest':\n",
    "        n_estimators = trial.suggest_int('n_estimators',100,1000)\n",
    "        max_depth = trial.suggest_int('max_depth',10,100)\n",
    "        min_samples_split = trial.suggest_int('min_sample_split',2,50)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf',1,100)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth,min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "\n",
    "    if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    model.fit(train_data['samples'].head(15000).tolist(), train_data['labels'].head(15000).tolist())\n",
    "    return model_performance(model)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "\n",
    "params = ''\n",
    "for key, value in study.best_trial.params.items():\n",
    "    # Concatenate the key-value pair to the result string\n",
    "    params += f'{key}: {value}, '\n",
    "\n",
    "writer.add_text('Best Hyper Parameters On LBPF',params)\n",
    "model_lbpf = classifier.train_model(study.best_trial.params,train_data['samples'].tolist(),train_data['labels'].tolist())\n",
    "results = classifier.test_model(model_lbpf,test_data['samples'].tolist(),test_data['labels'].tolist())\n",
    "writer.add_text('Results On LBPF',results)\n",
    "\n",
    "print(f'Results obtain with: {params}')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Multiscale wavelet Features\n",
    "\n",
    "Below we will try to obtain MultiScale wavelet features then with the help of optuna libray find the best performing model along with the best, Then evaluate the chosed model with chosen parametes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSWF = NlevelWaveletDecomposition(level=3,filter_type='haar').compute_wavelet_statistics\n",
    "\n",
    "classifier = Classification(MSWF,'MSWF',Configuration['PositiveSamples'],Configuration['NegativeSamples'],writer=writer)\n",
    "\n",
    "train_data,test_data = classifier.load_samples(['French'])\n",
    "print('data loaded')\n",
    "\n",
    "classifier.save_data(Configuration['Destination_train'],train_data)\n",
    "classifier.save_data(Configuration['Destination_test'],test_data)\n",
    "print('Data saved')\n",
    "\n",
    "classifier.get_data_distributions(train_data,test_data)\n",
    "print('distribution achieved')\n",
    "\n",
    "training_data_scaled, test_data_scaled = classifier.standardize_data(train_data,test_data)\n",
    "\n",
    "train_data['samples'] = training_data_scaled\n",
    "test_data['samples'] = test_data_scaled\n",
    "print('data Standarized')\n",
    "\n",
    "def model_performance(model):\n",
    "    \"\"\"\n",
    "    Get accuracy score on validation/test data from a trained model\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(test_data['samples'].tolist())\n",
    "    return round(accuracy_score(y_pred, test_data['labels'].tolist()),3)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    model = 'svm'\n",
    "    model_type = trial.suggest_categorical('model_type', ['randomforest', 'svm'])\n",
    "    if model_type == 'svm':\n",
    "        kernel = trial.suggest_categorical('kernel', ['poly', 'rbf', 'sigmoid'])\n",
    "        C = trial.suggest_float('C', 10, 210,step=10)\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"auto\", \"scale\"])\n",
    "\n",
    "        model = SVC(kernel=kernel, C=C,gamma=gamma)\n",
    "\n",
    "    if model_type == 'randomforest':\n",
    "        n_estimators = trial.suggest_int('n_estimators',100,1000)\n",
    "        max_depth = trial.suggest_int('max_depth',10,100)\n",
    "        min_samples_split = trial.suggest_int('min_sample_split',2,50)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf',1,100)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth,min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "\n",
    "    if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    model.fit(train_data['samples'].head(15000).tolist(), train_data['labels'].head(15000).tolist())\n",
    "    return model_performance(model)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "\n",
    "params = ''\n",
    "for key, value in study.best_trial.params.items():\n",
    "    # Concatenate the key-value pair to the result string\n",
    "    params += f'{key}: {value}, '\n",
    "\n",
    "\n",
    "writer.add_text('Best Hyper Parameters On MultiScaleWavelet',params)\n",
    "model_mswf = classifier.train_model(study.best_trial.params,train_data['samples'].tolist(),train_data['labels'].tolist())\n",
    "results = classifier.test_model(model_mswf,test_data['samples'].tolist(),test_data['labels'].tolist())\n",
    "writer.add_text('Ressults  On MultiScaleWavelet',results)\n",
    "\n",
    "print(f'Results obtain with: {params}')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Markov DCT Features\n",
    "\n",
    "Below we will try to obtain Markov DCT features then with the help of optuna libray find the best performing model along with the best, Then evaluate the chosed model with chosen parametes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKOV = MarkovFeatures(threshold=3).compute_markov_features\n",
    "\n",
    "classifier = Classification(MARKOV,'MARKOV',Configuration['PositiveSamples'],Configuration['NegativeSamples'],writer=writer)\n",
    "\n",
    "train_data,test_data = classifier.load_samples(['French'])\n",
    "print('data loaded')\n",
    "\n",
    "classifier.save_data(Configuration['Destination_train'],train_data)\n",
    "classifier.save_data(Configuration['Destination_test'],test_data)\n",
    "print('Data saved')\n",
    "\n",
    "classifier.get_data_distributions(train_data,test_data)\n",
    "print('distribution achieved')\n",
    "\n",
    "\n",
    "training_data_scaled, test_data_scaled = classifier.standardize_data(train_data,test_data)\n",
    "train_data['samples'] = training_data_scaled\n",
    "test_data['samples'] = test_data_scaled\n",
    "print('data Standarized')\n",
    "\n",
    "\n",
    "def model_performance(model):\n",
    "    \"\"\"\n",
    "    Get accuracy score on validation/test data from a trained model\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(test_data['samples'].tolist())\n",
    "    return round(accuracy_score(y_pred, test_data['labels'].tolist()),3)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    model = 'svm'\n",
    "    model_type = trial.suggest_categorical('model_type', ['randomforest', 'svm'])\n",
    "    if model_type == 'svm':\n",
    "        kernel = trial.suggest_categorical('kernel', ['poly', 'rbf', 'sigmoid'])\n",
    "        C = trial.suggest_float('C', 10, 210,step=10)\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"auto\", \"scale\"])\n",
    "\n",
    "        model = SVC(kernel=kernel, C=C,gamma=gamma)\n",
    "\n",
    "    if model_type == 'randomforest':\n",
    "        n_estimators = trial.suggest_int('n_estimators',100,1000)\n",
    "        max_depth = trial.suggest_int('max_depth',10,100)\n",
    "        min_samples_split = trial.suggest_int('min_sample_split',2,50)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf',1,100)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth,min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "\n",
    "    if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    model.fit(train_data['samples'].head(15000).tolist(), train_data['labels'].head(15000).tolist())\n",
    "    return model_performance(model)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "params = ''\n",
    "for key, value in study.best_trial.params.items():\n",
    "    # Concatenate the key-value pair to the result string\n",
    "    params += f'{key}: {value}, '\n",
    "\n",
    "\n",
    "writer.add_text('Best Hyper Parameters On MarkovDCT',params)\n",
    "model_markov = classifier.train_model(study.best_trial.params,train_data['samples'].tolist(),train_data['labels'].tolist())\n",
    "results = classifier.test_model(model_markov,test_data['samples'].tolist(),test_data['labels'].tolist())\n",
    "writer.add_text('Ressults On MarkovDCT',results)\n",
    "\n",
    "print(f'Results obtain with: {params}')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Resnet18 Features\n",
    "\n",
    "Below we will try to obtain Resnet18 features then with the help of optuna libray find the best performing model along with the best, Then evaluate the chosed model with chosen parametes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_name = 'ResNet18'\n",
    "ResNet = ResnetFeatures(model_name, Configuration['device']).get_features\n",
    "\n",
    "classifier = Classification(ResNet,model_name,Configuration['PositiveSamples'],Configuration['NegativeSamples'],writer=writer)\n",
    "\n",
    "train_data,test_data = classifier.load_samples(['French'])\n",
    "print('data loaded')\n",
    "\n",
    "classifier.save_data(Configuration['Destination_train'],train_data)\n",
    "classifier.save_data(Configuration['Destination_test'],test_data)\n",
    "print('Data saved')\n",
    "\n",
    "display(train_data.head(10))\n",
    "display(test_data.head(10))\n",
    "\n",
    "classifier.get_data_distributions(train_data,test_data)\n",
    "print('distribution achieved')\n",
    "\n",
    "def model_performance(model):\n",
    "    \"\"\"\n",
    "    Get accuracy score on validation/test data from a trained model\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(test_data['samples'].tolist())\n",
    "    return round(accuracy_score(y_pred, test_data['labels'].tolist()),3)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    model = 'svm'\n",
    "    model_type = trial.suggest_categorical('model_type', ['randomforest', 'svm'])\n",
    "    if model_type == 'svm':\n",
    "        kernel = trial.suggest_categorical('kernel', ['poly', 'rbf', 'sigmoid'])\n",
    "        C = trial.suggest_float('C', 10, 210,step=10)\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"auto\", \"scale\"])\n",
    "\n",
    "        model = SVC(kernel=kernel, C=C,gamma=gamma)\n",
    "\n",
    "    if model_type == 'randomforest':\n",
    "        n_estimators = trial.suggest_int('n_estimators',100,1000)\n",
    "        max_depth = trial.suggest_int('max_depth',10,100)\n",
    "        min_samples_split = trial.suggest_int('min_sample_split',2,50)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf',1,100)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth,min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "\n",
    "    if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    model.fit(train_data['samples'].head(10000).tolist(), train_data['labels'].head(10000).tolist())\n",
    "    return model_performance(model)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "\n",
    "params = ''\n",
    "for key, value in study.best_trial.params.items():\n",
    "    # Concatenate the key-value pair to the result string\n",
    "    params += f'{key}: {value}, '\n",
    "\n",
    "writer.add_text('Best Hyper Parameters On Resnet18',params)\n",
    "model_resnet18 = classifier.train_model(study.best_trial.params,train_data['samples'].tolist(),train_data['labels'].tolist())\n",
    "results = classifier.test_model(model_resnet18,test_data['samples'].tolist(),test_data['labels'].tolist())\n",
    "writer.add_text('Ressults On Resnet18',results)\n",
    "\n",
    "print(f'Results obtain with: {params}')\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Resnet50 Features\n",
    "\n",
    "Below we will try to obtain Resnet50 features then with the help of optuna libray find the best performing model along with the best, Then evaluate the chosed model with chosen parametes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_name = 'ResNet50'\n",
    "ResNet = ResnetFeatures(model_name,Configuration['device']).get_features\n",
    "\n",
    "classifier = Classification(ResNet,model_name,Configuration['PositiveSamples'],Configuration['NegativeSamples'],writer)\n",
    "\n",
    "train_data,test_data = classifier.load_samples(['French'])\n",
    "print('data loaded')\n",
    "\n",
    "classifier.save_data(Configuration['Destination_train'],train_data)\n",
    "classifier.save_data(Configuration['Destination_test'],test_data)\n",
    "print('Data saved')\n",
    "\n",
    "display(train_data.head(10))\n",
    "display(test_data.head(10))\n",
    "\n",
    "classifier.get_data_distributions(train_data,test_data)\n",
    "print('distribution achieved')\n",
    "\n",
    "\n",
    "def model_performance(model):\n",
    "    \"\"\"\n",
    "    Get accuracy score on validation/test data from a trained model\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(test_data['samples'].tolist())\n",
    "    return round(accuracy_score(y_pred, test_data['labels'].tolist()),3)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    model = 'svm'\n",
    "    model_type = trial.suggest_categorical('model_type', ['randomforest', 'svm'])\n",
    "    if model_type == 'svm':\n",
    "        kernel = trial.suggest_categorical('kernel', ['poly', 'rbf', 'sigmoid'])\n",
    "        C = trial.suggest_float('C', 10, 310,step=10)\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"auto\", \"scale\"])\n",
    "\n",
    "        model = SVC(kernel=kernel, C=C,gamma=gamma)\n",
    "\n",
    "    if model_type == 'randomforest':\n",
    "        n_estimators = trial.suggest_int('n_estimators',100,1000)\n",
    "        max_depth = trial.suggest_int('max_depth',10,100)\n",
    "        min_samples_split = trial.suggest_int('min_sample_split',2,50)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf',1,100)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth,min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "\n",
    "    if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    model.fit(train_data['samples'].head(10000).tolist(), train_data['labels'].head(10000).tolist())\n",
    "    return model_performance(model)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "params = ''\n",
    "for key, value in study.best_trial.params.items():\n",
    "    # Concatenate the key-value pair to the result string\n",
    "    params += f'{key}: {value}, '\n",
    "\n",
    "params = ''\n",
    "for key, value in study.best_trial.params.items():\n",
    "    # Concatenate the key-value pair to the result string\n",
    "    params += f'{key}: {value}, '\n",
    "\n",
    "writer.add_text('Best Hyper Parameters On Resnet50',params)\n",
    "model_resnet50 = classifier.train_model(study.best_trial.params,train_data['samples'].tolist(),train_data['labels'].tolist())\n",
    "results = classifier.test_model(model_resnet50,test_data['samples'].tolist(),test_data['labels'].tolist())\n",
    "writer.add_text('Ressults On Resnet50',results)\n",
    "\n",
    "print(f'Results obtain with: {params}')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN this seciton we will Test our trained models on on DLC dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On LBPF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = r'C:\\Users\\Utente\\Projects\\Thesis\\Evaluations\\Machine Learning Based\\Extracted_features_dlc'\n",
    "\n",
    "LBPF = MultiScaleLocalBinaryPatternFeatures(Configuration['LBPF_parameters']).Calculate_MultiScale_LBPF\n",
    "\n",
    "classifier = Classification(LBPF,'LBPF',Configuration['PostiveSamples_dlc'],Configuration['NegativeSamples_dlc'],writer)\n",
    "\n",
    "data,_ = classifier.load_samples(['French'])\n",
    "\n",
    "classifier.save_data(r\"C:\\Users\\Utente\\Projects\\Thesis\\Evaluations\\Machine Learning Based\\Extracted_features_dlc\",data)\n",
    "\n",
    "results = classifier.test_model(model_lbpf,data['samples'].tolist(),data['labels'].tolist())\n",
    "\n",
    "writer.add_text('Results On dlc features LBPF', results)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On MSWF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSWF = NlevelWaveletDecomposition(level=3,filter_type='haar').compute_wavelet_statistics\n",
    "\n",
    "classifier = Classification(MSWF,'MSWF',Configuration['PostiveSamples_dlc'],Configuration['NegativeSamples_dlc'],writer)\n",
    "\n",
    "data,_ = classifier.load_samples(['French'])\n",
    "\n",
    "classifier.save_data(r\"C:\\Users\\Utente\\Projects\\Thesis\\Evaluations\\Machine Learning Based\\Extracted_features_dlc\",data)\n",
    "\n",
    "results = classifier.test_model(model_mswf,data['samples'].tolist(),data['labels'].tolist())\n",
    "\n",
    "writer.add_text('Results On dlc features MSWF', results)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On MARKOV Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MARKOV'\n",
    "\n",
    "MARKOV = MarkovFeatures(threshold=3).compute_markov_features\n",
    "\n",
    "classifier = Classification(MARKOV,model_name,Configuration['PostiveSamples_dlc'],Configuration['NegativeSamples_dlc'],writer)\n",
    "\n",
    "data,_ = classifier.load_samples(['French'])\n",
    "\n",
    "classifier.save_data(r\"C:\\Users\\Utente\\Projects\\Thesis\\Evaluations\\Machine Learning Based\\Extracted_features_dlc\",data)\n",
    "\n",
    "results = classifier.test_model(model_markov,data['samples'].tolist(),data['labels'].tolist())\n",
    "\n",
    "writer.add_text('Results On dlc features Markov', results)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On RESNET18 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ResNet18'\n",
    "\n",
    "RESNET = ResnetFeatures(model_name,Configuration['device']).get_features\n",
    "\n",
    "classifier = Classification(RESNET,model_name,Configuration['PostiveSamples_dlc'],Configuration['NegativeSamples_dlc'],writer)\n",
    "\n",
    "data, _ = classifier.load_samples(['French'])\n",
    "\n",
    "results = classifier.test_model(model_resnet18,data['samples'].tolist(),data['labels'].tolist())\n",
    "\n",
    "writer.add_text('Results On dlc features Resnet18', results)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On RESNET50 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ResNet50'\n",
    "\n",
    "RESNET = ResnetFeatures(model_name,Configuration['device']).get_features\n",
    "\n",
    "classifier = Classification(RESNET,model_name,Configuration['PostiveSamples_dlc'],Configuration['NegativeSamples_dlc'], writer)\n",
    "\n",
    "data, _ = classifier.load_samples(['French'])\n",
    "\n",
    "results = classifier.test_model(model_resnet50,data['samples'].tolist(),data['labels'].tolist())\n",
    "\n",
    "writer.add_text('Results On dlc features Resnet50', results)\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisDemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
